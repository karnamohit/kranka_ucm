{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/karnamohit/kranka_ucm/blob/master/tf1_test_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rtgEx4iIVXly"
   },
   "source": [
    "Importing all the useful libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_vn7WnlmKCd"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "zGuZTN4AhVKa",
    "outputId": "1a3a06e4-f65f-487c-de00-f825a8229cba"
   },
   "outputs": [],
   "source": [
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jYKUs0zghlEf"
   },
   "outputs": [],
   "source": [
    "#!ls /content/drive/My\\ Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8knKLFRVFvY"
   },
   "source": [
    "Checking the version of TensorFlow, NumPy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "HY5Y0g3iqqkc",
    "outputId": "bd7160ce-e08b-457b-d0b3-496ba76c404b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version info:\t 1.14.0\n",
      " \n",
      "------------------------------------------------------------\n",
      " \n",
      "NumPy version info:\t \t 1.16.4\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version info:\\t',tf.__version__)\n",
    "#!pip show tensorflow\n",
    "print(' ')\n",
    "print('------------------------------------------------------------')\n",
    "print(' ')\n",
    "#!pip show numpy\n",
    "print('NumPy version info:\\t \\t',np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9sX69fS2xhhK"
   },
   "source": [
    "**Making, processing raw data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yirYUEPUb1Sd"
   },
   "source": [
    "Make up data (2x2, sequentially indexed matrices)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcAFC9e9WE93"
   },
   "outputs": [],
   "source": [
    "series_data = np.zeros((100,2,2), np.float64)        # \"time\"-series data for a 2x2 evolving matrix with 100 time-steps, randomly initialized\n",
    "for i in range(series_data.shape[0]):\n",
    "    #series_data[i,:,:] = np.sqrt(i)*3 + 5\n",
    "    #series_data[i,:,:] = np.exp(1/np.sqrt(i+1))*3 + 5\n",
    "    series_data[i,:,:] = np.log(np.sqrt(i+1))*i + 5\n",
    "#print(series_data)\n",
    "tsteps = int(series_data.shape[0])         # total number of time-steps in the series\n",
    "tsteps_train = int(tsteps/2)               # number of time-steps used for training\n",
    "mat_row = int(series_data.shape[1])\n",
    "mat_col = int(series_data.shape[2])\n",
    "\n",
    "#print(series_data[tsteps_train-2,:,:])\n",
    "#print(series_data[tsteps_train-1,:,:])\n",
    "#print(series_data[tsteps_train,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dn0h4QC2E43v"
   },
   "source": [
    "Define a sequence (multiple consecutive 2x2 matrices) size, ```len_seq```, to be fed in as input (equivalent to the amount of memory in previous time-steps, in our case), and the amount of overlap, ```n_overlap``` between any two adjacent input sequences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27iXOLgAPEzr"
   },
   "outputs": [],
   "source": [
    "n_pred = 1 # number of steps to be predicted\n",
    "len_seq = 3 # length of input-sequence\n",
    "n_overlap = 2 # overlap window between consecutive sequences\n",
    "n_seq = int((tsteps_train - (len_seq - n_overlap)) / (len_seq - n_overlap)) # number of sequences of size len_seq\n",
    "\n",
    "if (n_overlap >= len_seq):\n",
    "    raise Exception(\"n_overlap must be less than len_seq\")\n",
    "else:\n",
    "    if (n_pred >= (tsteps_train - (len_seq - 1))):\n",
    "        raise Exception(\"n_pred must be MUCH less than the difference b/w the total time-steps and the length of the input sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJcC3-emQOo6"
   },
   "source": [
    "Build tensor of sequences, call it ```seq_series_data```..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "H9ewF3MOQdiJ",
    "outputId": "c7f453bd-9b18-441d-e2b3-7a6dacfb05fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 3, 2, 2)\n",
      "(49, 1, 2, 2)\n",
      "(49, 3, 4)\n",
      "(49, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "seq_series_data = np.zeros((n_seq, len_seq, mat_row, mat_col), np.float64)\n",
    "seq_series_data_pred = np.zeros((n_seq, n_pred, mat_row, mat_col), np.float64)\n",
    "\n",
    "i = 0\n",
    "k = 0\n",
    "\n",
    "while (i < tsteps and k < n_seq):       # the training sequences start at time-step 1 (series_data[0] element) in this case\n",
    "    try:\n",
    "        for j in range(len_seq):\n",
    "            seq_series_data[k,j,:,:] = series_data[(i+j),:,:]\n",
    "            seq_series_data_pred[k,0:(1+n_pred),:,:] = series_data[(i+j+1):(i+j+n_pred+1),:,:]\n",
    "    except IndexError:\n",
    "        raise Exception(\"ummm... Houston, we've had a problem.\")\n",
    "    #seq_series_data_pred = np.append(seq_series_data_pred, series_data[(i+j+1):(i+j+1+n_pred),:,:], axis = 1)\n",
    "    i += len_seq - n_overlap\n",
    "    k += 1\n",
    "    #print(k,j,i)\n",
    "    #print(seq_series_data[k,:,:,:])\n",
    "\n",
    "#seq_series_data_pred = np.delete(seq_series_data_pred, seq_series_data_pred[0,:,:,:], axis=0) # true output\n",
    "\n",
    "x_inp = np.reshape(seq_series_data[:,:len_seq,:,:], [n_seq, len_seq, (mat_row*mat_col)])\n",
    "y_tru = np.reshape(seq_series_data_pred[:,:,:,:], [n_seq, n_pred, (mat_row*mat_col)])\n",
    "\n",
    "\n",
    "#print(seq_series_data_pred)\n",
    "#print(type(seq_series_data))\n",
    "#print(n_seq)\n",
    "print(seq_series_data.shape)\n",
    "print(seq_series_data_pred.shape)\n",
    "#print(seq_series_data_pred[-1,:,:,:])\n",
    "print(x_inp.shape)\n",
    "print(y_tru.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "LcF6XmVOo4ao",
    "outputId": "b2af00fc-fcc7-426a-e410-df9c7343d5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X (excluding the true output):  (2, 2, 2)\n",
      "no. of batches of X:  49\n",
      "shape of Y_pred and Y_true:  (1, 1, 2, 2)\n",
      "no. of predictions (equal to the no. of batches of X):  49\n",
      "shape of X (excluding the true output):  (2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "# test to check data-type and shape of seq_series_data, seq_series_data_pred, and a slice seq_series_data\n",
    "\n",
    "print('shape of X (excluding the true output): ',seq_series_data[0,:-1,:,:].shape) \n",
    "print('no. of batches of X: ', seq_series_data.shape[0]) # 1 - prints the number of time-steps, each of these associated with a sequence of 2x2 matrices\n",
    "#print(seq_series_data[0].shape)\n",
    "print('shape of Y_pred and Y_true: ', seq_series_data_pred[:1,:,:].shape)\n",
    "print('no. of predictions (equal to the no. of batches of X): ', seq_series_data_pred.shape[0]) \n",
    "            # 2 - prints the number of time-steps, each of these assocaited with one 2x2 matrix (model truth), must match 1 above\n",
    "print('shape of X (excluding the true output): ', seq_series_data[0,:-1,:,:].shape) # prints the sequence-size used per prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkAJe24dJM9v"
   },
   "source": [
    "**Building a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NI2T2lW7gNzA"
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBOJKKq3jlE3"
   },
   "source": [
    "Define activation functions, operations, etc. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "8c7Z_jERjo_q",
    "outputId": "c2c5c28e-2a77-4124-a341-774261ee69cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# using a convolution operation (with a custom filter), conv_3D, for hidden layer h_2\\n\\n# define a custom filter, filter_1, to be used with the one-shot (stride-less) convolution operation, conv_3D_oneshot\\n#     ignoring padding for now\\ndef filter_1(d, h, w):\\n    # the latest sequence gets the highest contribution\\n    filter = np.zeros((d, h, w), np.float64)\\n    for i in range(d):\\n        for j in range(h):\\n            for k in range(w):\\n                filter[i,j,k] = np.sqrt(np.exp(d))\\n    return tf.convert_to_tensor(filter, dtype=tf.int32)\\n\\n# define a (stride-less) function for the convolution operation\\ndef conv_3D_oneshot(z, name=\"custom_conv_3D_1\"):\\n    shape = z.get_shape().as_list()\\n    depth = shape[0].value\\n    height = shape[1].value\\n    width = shape[2].value\\n    filter = filter_1(depth, height, width)\\n    return tf.math.reduce_sum(tf.math.multiply(z, filter), 0)'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using \"activation=None\" (linear operation) in tf.layers.dense for hidden layer h_1\n",
    "\n",
    "def softplus(z, name=\"softplus\"):\n",
    "    return tf.math.softplus(z)\n",
    "\n",
    "def exponential(z, name=\"exponential\"):\n",
    "    return tf.math.exp(z)\n",
    "\n",
    "def square(z, name=\"square\"):\n",
    "    return tf.math.square(z)\n",
    "\n",
    "def expinv(z, name=\"expinv\"):\n",
    "    return tf.math.exp(-z)\n",
    "\n",
    "def inv(z, name=\"inv\"):\n",
    "    return tf.math.square(tf.math.sqrt(1/z))\n",
    "\n",
    "'''# using a convolution operation (with a custom filter), conv_3D, for hidden layer h_2\n",
    "\n",
    "# define a custom filter, filter_1, to be used with the one-shot (stride-less) convolution operation, conv_3D_oneshot\n",
    "#     ignoring padding for now\n",
    "def filter_1(d, h, w):\n",
    "    # the latest sequence gets the highest contribution\n",
    "    filter = np.zeros((d, h, w), np.float64)\n",
    "    for i in range(d):\n",
    "        for j in range(h):\n",
    "            for k in range(w):\n",
    "                filter[i,j,k] = np.sqrt(np.exp(d))\n",
    "    return tf.convert_to_tensor(filter, dtype=tf.int32)\n",
    "\n",
    "# define a (stride-less) function for the convolution operation\n",
    "def conv_3D_oneshot(z, name=\"custom_conv_3D_1\"):\n",
    "    shape = z.get_shape().as_list()\n",
    "    depth = shape[0].value\n",
    "    height = shape[1].value\n",
    "    width = shape[2].value\n",
    "    filter = filter_1(depth, height, width)\n",
    "    return tf.math.reduce_sum(tf.math.multiply(z, filter), 0)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8sLsw1_jcKf"
   },
   "source": [
    "Define architectural (hyper-)parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yritMjt8pX2n"
   },
   "outputs": [],
   "source": [
    "# define layers with #(nodes/layer), n_units_i, connected by weight matrices, w_i, and bias matrices, b_i\n",
    "\n",
    "n_inp_n = seq_series_data.shape[0]   # no. of 2x2 matrices fed as input (\"units\" per INPUT layer)\n",
    "n_units_1 = 8   # no. of units in the first hidden layer\n",
    "#n_units_2 = 8\n",
    "#n_units_3 = 4\n",
    "n_units_1_conv = n_units_1\n",
    "            # no. of (convolution) units in the output layer; must equal the number of units in the immediately preceding hidden layer\n",
    "n_outp_n  = n_pred\n",
    "            # number of (2x2) tensors the model must output (the same size as the sequence-size of the model output, 1 in the current case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DCl_pY3wjNNq",
    "outputId": "99b53aad-ebc2-4acd-cc89-fe7bf78cbb85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inp_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "it09yTY1oP4l"
   },
   "source": [
    "Construct input placeholders for the model, build the layers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmCu5XbAoPN1"
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=(None, len_seq, (mat_row*mat_col)), name='input')    # raw input (?x3x4)\n",
    "'''\n",
    "Y_pred = tf.placeholder(tf.float64, shape=(None, mat_row, mat_col),name='model_output')\n",
    "            # model output: predicting a 2x2 matrix \"?\" time-step(s) at a time\n",
    "'''\n",
    "Y_true = tf.placeholder(tf.float64, shape=(None, n_pred, (mat_row*mat_col)), name='true_output')\n",
    "      # true output: same shape as the model output\n",
    "\n",
    "with tf.name_scope(\"ann_1\"):    # test ANN model, \"ann_1\"\n",
    "    h_1 = tf.layers.dense(X, n_units_1, activation=tf.nn.relu, use_bias=True, name=\"hidden_layer_1\")\n",
    "          # first hidden layer (linear activation)\n",
    "    #h_2 = tf.layers.dense(h_1, n_units_2, activation=None, use_bias=False, name=\"hidden_layer_2\")\n",
    "    #h_3 = tf.layers.dense(h_2, n_units_3, activation=None, use_bias=False, name=\"hidden_layer_3\")\n",
    "    f_1 = tf.get_variable(\"inp_to_outp_1\", [n_units_1, n_outp_n], trainable=True, dtype=tf.float64)\n",
    "          # \"filter\" with zero strides, for \"one-shot convolution\"\n",
    "    hf_1 = tf.squeeze(tf.tensordot(h_1, f_1, axes = 1, name=\"hidden_funnel_layer_1\"), 2)\n",
    "          # funneling layer, to reduce input dimensions to output dimensions \n",
    "    f_o = tf.get_variable(\"hf_to_outp_1\", [len_seq, n_outp_n, 1], trainable=True, dtype=tf.float64)\n",
    "    hf_o = tf.tensordot(hf_1, f_o, axes = 1, name=\"output_funnel_layer\")    # output funnel layer\n",
    "    Y_pred = tf.layers.dense(hf_o, (mat_row*mat_col), activation=None, use_bias=True, name=\"output_layer\")    # model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "GS2SKRLHelL_",
    "outputId": "a2fb5bb9-9127-4556-8a68-16269e046c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9O5JuU9wQbFm"
   },
   "source": [
    "Define the loss function; choose the optimization algorithm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dTnlBToaItF"
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss_ann_1\"):   # loss function for the model \"ann_1\"\n",
    "    loss = tf.losses.mean_squared_error(Y_pred, Y_true)   # element-wise MSE\n",
    "\n",
    "learn_rate = 0.001    # hyper-parameter for updating a variable v: v_new = v_old + delta_v = v_old - learn_rate*grad_v(loss)\n",
    "\n",
    "with tf.name_scope(\"opt_ann_1\"):    # optimizer for \"ann_1\"\n",
    "    opt = tf.train.GradientDescentOptimizer(learn_rate)   # use the gradient descent optimization algorithm\n",
    "    loss_min = opt.minimize(loss)   # operation to minimize \"loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CltqEejDeG8"
   },
   "source": [
    "**Running the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "A7BJ2Icsv4SK",
    "outputId": "6c14edc1-cc53-4fc1-e0f0-3ac93855e035"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(series_data.shape)\\ninp = np.expand_dims(series_data[:99,:,:],-1)\\nout = series_data[1:,:,:]\\nprint(inp.shape)\\nprint(out.shape)'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(series_data.shape)\n",
    "inp = np.expand_dims(series_data[:99,:,:],-1)\n",
    "out = series_data[1:,:,:]\n",
    "print(inp.shape)\n",
    "print(out.shape)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2ZCOTsCB_w1"
   },
   "source": [
    "Assign no. of training cycles, batch-sizes, etc.; set up initializer and session-saver objects ```init``` and ```saver``` (required for assigning initial values to the ```tf.Graph``` variables and saving the values of variables optimized in the session); use a context manager to run the ```tf.Session```..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CO4BdO_CB1am",
    "outputId": "4ba66d6a-8eeb-46d3-f184-3ed2227dd7f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MSE (training): 3663.9312 \n",
      " [[ -1.98136778   0.14254071   9.11909833 -10.74418232]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "1000 MSE (training): 1877.923 \n",
      " [[20.49718656 20.49710049 20.49673671 20.49754166]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "2000 MSE (training): 1248.8119 \n",
      " [[32.91159502 32.91154284 32.91132223 32.91181037]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "3000 MSE (training): 1017.43286 \n",
      " [[40.44037282 40.44034119 40.44020738 40.44050339]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "4000 MSE (training): 932.3345 \n",
      " [[45.00623637 45.00621718 45.00613604 45.00631554]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "5000 MSE (training): 901.03644 \n",
      " [[47.77522632 47.77521468 47.77516547 47.77527431]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "6000 MSE (training): 889.5255 \n",
      " [[49.4544936  49.45448652 49.45445668 49.45452269]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "7000 MSE (training): 885.29175 \n",
      " [[50.4728933  50.47288904 50.47287093 50.47291095]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "8000 MSE (training): 883.7347 \n",
      " [[51.0905067  51.09050414 51.09049315 51.09051741]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "9000 MSE (training): 883.162 \n",
      " [[51.46506133 51.4650598  51.46505309 51.46506782]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "10000 MSE (training): 882.95135 \n",
      " [[51.69221184 51.69221086 51.69220679 51.69221571]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "11000 MSE (training): 882.8739 \n",
      " [[51.82996831 51.82996771 51.82996525 51.82997065]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "12000 MSE (training): 882.8455 \n",
      " [[51.9135114  51.91351106 51.91350952 51.9135128 ]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "13000 MSE (training): 882.83496 \n",
      " [[51.96417647 51.96417626 51.96417538 51.96417735]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "14000 MSE (training): 882.8312 \n",
      " [[51.99490257 51.99490245 51.99490193 51.9949031 ]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "15000 MSE (training): 882.8297 \n",
      " [[52.01353655 52.01353648 52.01353618 52.01353686]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "16000 MSE (training): 882.8291 \n",
      " [[52.02483723 52.02483719 52.02483702 52.02483742]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "17000 MSE (training): 882.829 \n",
      " [[52.03169059 52.03169056 52.03169045 52.03169069]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "18000 MSE (training): 882.8289 \n",
      " [[52.03584681 52.03584679 52.03584674 52.03584689]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "19000 MSE (training): 882.8291 \n",
      " [[52.0383674  52.03836739 52.03836736 52.03836745]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "20000 MSE (training): 882.8289 \n",
      " [[52.03989603 52.03989602 52.03989601 52.03989606]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "21000 MSE (training): 882.829 \n",
      " [[52.04082306 52.04082306 52.04082305 52.04082307]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "22000 MSE (training): 882.8289 \n",
      " [[52.04138526 52.04138527 52.04138526 52.04138527]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "23000 MSE (training): 882.8289 \n",
      " [[52.04172622 52.04172622 52.04172622 52.04172622]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "24000 MSE (training): 882.8289 \n",
      " [[52.04193299 52.04193299 52.04193299 52.04193299]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "25000 MSE (training): 882.8289 \n",
      " [[52.04205839 52.04205839 52.04205838 52.04205839]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "26000 MSE (training): 882.829 \n",
      " [[52.04213442 52.04213442 52.04213442 52.04213442]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "27000 MSE (training): 882.8289 \n",
      " [[52.04218055 52.04218055 52.04218055 52.04218055]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "28000 MSE (training): 882.8289 \n",
      " [[52.0422085 52.0422085 52.0422085 52.0422085]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "29000 MSE (training): 882.8289 \n",
      " [[52.04222544 52.04222544 52.04222544 52.04222544]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "30000 MSE (training): 882.82886 \n",
      " [[52.04223571 52.04223571 52.04223571 52.04223571]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "31000 MSE (training): 882.82886 \n",
      " [[52.04224208 52.04224208 52.04224208 52.04224208]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "32000 MSE (training): 882.82886 \n",
      " [[52.04224591 52.04224591 52.04224591 52.04224591]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "33000 MSE (training): 882.82886 \n",
      " [[52.04224791 52.04224791 52.04224791 52.04224791]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "34000 MSE (training): 882.82886 \n",
      " [[52.04224968 52.04224968 52.04224968 52.04224968]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "35000 MSE (training): 882.82886 \n",
      " [[52.04224968 52.04224968 52.04224968 52.04224968]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "36000 MSE (training): 882.82886 \n",
      " [[52.04224968 52.04224968 52.04224968 52.04224968]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "37000 MSE (training): 882.82886 \n",
      " [[52.04224968 52.04224968 52.04224968 52.04224968]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "38000 MSE (training): 882.82886 \n",
      " [[52.04224968 52.04224968 52.04224968 52.04224968]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "39000 MSE (training): 882.82886 \n",
      " [[52.04224968 52.04224968 52.04224968 52.04224968]] [[105.75671482 105.75671482 105.75671482 105.75671482]]\n",
      "<tf.Variable 'hidden_layer_1/kernel:0' shape=(4, 8) dtype=float64_ref>\n",
      "[[-0.4105664  -0.51629569 -0.6036214  -0.19475424 -0.37317062 -0.57297636\n",
      "  -0.20320476 -0.48030229]\n",
      " [-0.49316789 -0.37436302  0.573448    0.40903787 -0.31699502  0.13488277\n",
      "  -0.43127105 -0.43141808]\n",
      " [-0.1141494  -0.35598376 -0.083461    0.1805298  -0.03906685 -0.3533121\n",
      "   0.02413293  0.43088645]\n",
      " [-0.03743185 -0.64158695  0.09447874 -0.51073406  0.28891892  0.27912325\n",
      "  -0.41320802  0.45815347]]\n",
      "<tf.Variable 'hidden_layer_1/bias:0' shape=(8,) dtype=float64_ref>\n",
      "[ 0.          0.          0.          0.         -0.00296495  0.\n",
      " -0.00685044  0.        ]\n",
      "<tf.Variable 'inp_to_outp_1:0' shape=(8, 1) dtype=float64_ref>\n",
      "[[ 0.39947097]\n",
      " [ 0.10270044]\n",
      " [-0.21066431]\n",
      " [-0.38927695]\n",
      " [-0.03650296]\n",
      " [ 0.47934206]\n",
      " [-0.1386003 ]\n",
      " [-0.51214135]]\n",
      "<tf.Variable 'hf_to_outp_1:0' shape=(3, 1, 1) dtype=float64_ref>\n",
      "[[[-0.22447914]]\n",
      "\n",
      " [[ 0.39926216]]\n",
      "\n",
      " [[-0.56993304]]]\n",
      "<tf.Variable 'output_layer/kernel:0' shape=(1, 4) dtype=float64_ref>\n",
      "[[ 0.03897774  0.16140246  0.67882238 -0.46612178]]\n",
      "<tf.Variable 'output_layer/bias:0' shape=(4,) dtype=float64_ref>\n",
      "[52.04224968 52.04224968 52.04224968 52.04224968]\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 40000   # number of optimization steps on the WHOLE training dataset\n",
    "\n",
    "init = tf.global_variables_initializer()    # initialize the model variables\n",
    "saver = tf.train.Saver()    # saves the updated parameters from a session run\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \"\"\"#inp = np.expand_dims(series_data[:n_inp_n,:,:],-1)\n",
    "    #out = np.expand_dims(series_data[n_inp_n,:,:],0)\n",
    "    #n_outp_n = 3\n",
    "    n_len = len(series_data)\n",
    "    #print(n_len,n_outp_n)\n",
    "    inp = np.expand_dims(series_data[:(n_len - n_outp_n),:,:],-1)\n",
    "    out = np.reshape(series_data[(n_len - n_outp_n):,:,:], [1,mat_row,mat_col,n_outp_n])\n",
    "    print(inp.shape)\n",
    "    print(out.shape)\n",
    "    \"\"\"\n",
    "    y_out = Y_pred.eval(feed_dict={X: x_inp})\n",
    "    #print(y_out)\n",
    "    for train_epoch in range(n_epoch):\n",
    "        #for batch_iter in range(seq_series_data.shape[0]):\n",
    "        #print(inp.shape, out.shape)\n",
    "        #print(X_in.get_shape(),Y_pred.get_shape(), Y_true.get_shape(), hf_1.get_shape(), f_1.get_shape(), h_1.get_shape())\n",
    "        sess.run(loss_min, feed_dict={X: x_inp, Y_true: y_tru})\n",
    "        if (train_epoch % 1000 == 0):\n",
    "            y_out = Y_pred.eval(feed_dict={X: x_inp})\n",
    "            #layer_2 = hf_1.eval(feed_dict={X: inp})\n",
    "            #layer_3 = h_2.eval(feed_dict={X: inp})\n",
    "            acc_train = loss.eval(feed_dict={X: x_inp, Y_true: y_tru})\n",
    "            print(train_epoch, \"MSE (training):\", acc_train, \"\\n\", y_out[-1,:], y_tru[-1,:])#, \"\\n\", layer_2, \"\\n\", layer_3)\n",
    "\n",
    "    vars = tf.trainable_variables()\n",
    "    for v in vars:\n",
    "        print(v)\n",
    "        print(sess.run(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWcQbDHlmfSl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tf1_test_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf1_14_0_gpu",
   "language": "python",
   "name": "tf1_14_0_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
